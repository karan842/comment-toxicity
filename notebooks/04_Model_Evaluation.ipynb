{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Model Evaluation 🔍📊\n\n### In this kernel we'll look at how to evaluate *tf-keras* model for `multilabel-text` data.\n\n![Header Image](https://www.aihr.com/wp-content/uploads/Training-evaluation-cover-1000x553-1.png)","metadata":{}},{"cell_type":"markdown","source":"## About Proejct📜:\n\n- I'm currently learning `MLOps` for Deep Learning/BERT model so for that I picked up a text-data from Kaggle which `jigsaw-comment-classification-challange.`\n- I uploaded the code of whole ongoinh `comment-toxicity-detection project` on https://github.com/karan842/comment-toxicity\n- I created `DVC` based data cleaning pipeline which can clean the text data and saved them in `.csv` format.\n- Meanwhile, I started analyzing the text data to gain information about the comments and how they are i.e toxic, insulting, threating or all.\n- Perfomed EDA and plotted the charts, graphs using Python libraries such as `Matplotlib and Seaborn`. You can find text analytics notebook in `notebooks` folder on given repository.\n- After performing text data cleaning and Analyzing process and I started building a Tensorflow based neural network model which can detect the toxicity in the comment. For that I've `BERT` pre-trained model. You can also find that notebook on `notebooks` folder.\n- I trained the model on `Kaggle GPU P100`, although I've GPU in my system but I prefer Kaggle for acceleration. I saved the model and used it in `Flask` app under APIs.\n- I tested APIs using `Postman` and then built a dockerfile which can support `GPU and CUDNN` to run neural network model.\n- I'll deploy this FlasAPI on Cloud using using Github Actions.\n","metadata":{}},{"cell_type":"markdown","source":"## What is in this kernel🤔?\n\n- In this kernel we'll look at the model evaluation part for multilabel classification data.\n\n- *Model evaluation* is the process of using different evaluation metrics to understand a machine learning model's performance, as well as its strengths and weaknesses\n\n\n#### Let's revise some important concepts for model evaluation techniques on classification data.\n- Source: `ChatGPT`\n\n1. **Confustion Matrix**: A matrix that displays the number of true positive, true negative, false positive and false negative predictions made by a model.\n\n   1. **True Positive(TP)** represents the number of True Positives. This refers to the total number of observations that belong to the positive class and have been predicted correctly.\n   \n   2. **True Negative(TN)** represents the number of True Negative. This is the total number of observations that belong to the negative class and have been predicted correctly.\n   \n   3. **False Positive(FP)** also known as `Type 1` error. This is total number of observation that have been predicted to belong positive class, but instead they are actually belong to negative class\n   \n   4. **False Negative(FN)** also known as `Type 2` error. This is total number of observation that have been predicted to belong negative class, but instead belong to the positive class\n   \n \n2. **Accuracy**: The proportion of correctly classified instances out of the total number of instances. It is the most commonly used metric for classification problems, but it can be misleading if the classes are imbalanced. `TP/TP+FP`\n\n3. **Precision**: The proportion of true positive predictions out of all positive predictions made by the model. It measures how many of the positive predictions were actually correct.\n   \n3. **Recall** The proportion of true positive predictions out of all actual positive instances. It measures how well the model can find all the positive instances. TP/TP+FN\n\n4. **F1 Score**: The harmonic mean of precision and recall. It is a balance between precision and recall and can be used when both are important.\n\n**P.S: We're not using `AUC-ROC` metrices because thery useful in binary classification problem.**\n\n\n### What is Multi-label classification? How it is different than Single-label classification.\n\n- Multi-label classification is a type of supervised machine learning problem where each instance can be assigned multiple labels, rather than just one label in single-label classification. \n\n\n- **Key Differences**\n 1. **Label Independence**:  In single-label classification, the labels are mutually exclusive, meaning that an instance can only have one label. In multi-label classification, the labels are not mutually exclusive, meaning that an instance can have multiple labels.\n \n 2. **Output format**: In single-label classification, the output format is typically a single label or class. In multi-label classification, the output format is typically a binary vector or a probability vector, where each element represents the likelihood of a particular label being assigned to the instance.\n \n 3. **Evaluation Metrics**: Because of the different output format, the evaluation metrics for multi-label classification are also different from those of single-label classification. Some commonly used metrics include hamming loss, subset accuracy, jaccard similarity, and F1-score.\n\n\n### Hamming Losss:\n\nIt is a commonly used evaluation metric for measuring the performance of a classifier. It is defined as the proportion of labels that are incorrectly predicted. The value ranges from 0 (perfect prediction) to 1 (all labels are incorrectly predicted).\nAdvantages of using Hamming loss is that it penalizes all types of errors equally. In other words, it does not distinguish between false positives and false negatives. It is that it's easy to compute and understand. It doesn't take into account the order of the labels, and it doesn't consider the correlations between labels.\n\n### Jaccard Similarity\n\nIt is defined as the proportion of labels that are correctly predicted out of the total number of labels in the true and predicted sets. The Jaccard similarity for a dataset is the average Jaccard similarity over all instances in the dataset. The value ranges from 0 (no labels are correctly predicted) to 1 (all labels are correctly predicted). It penalizes predictions where too many labels are predicted, but rewards predictions where the correct labels are predicted. Jaccard similarity doesn't take into account false negatives, which means it only cares about the labels that were predicted regardless of the labels that were missed.","metadata":{}},{"cell_type":"code","source":"!pip install tensorflow_text==2.8.2","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-01-15T06:08:55.680714Z","iopub.execute_input":"2023-01-15T06:08:55.681198Z","iopub.status.idle":"2023-01-15T06:10:03.177774Z","shell.execute_reply.started":"2023-01-15T06:08:55.681158Z","shell.execute_reply":"2023-01-15T06:10:03.176615Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting tensorflow_text==2.8.2\n  Downloading tensorflow_text-2.8.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow_text==2.8.2) (0.12.0)\nCollecting tensorflow<2.9,>=2.8.0\n  Downloading tensorflow-2.8.4-cp37-cp37m-manylinux2010_x86_64.whl (497.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.9/497.9 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (1.21.6)\nRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (1.12.1)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (1.43.0)\nRequirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (3.19.4)\nCollecting tensorboard<2.9,>=2.8\n  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hRequirement already satisfied: gast>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (0.4.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (59.8.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (0.2.0)\nCollecting libclang>=9.0.1\n  Downloading libclang-15.0.6.1-py2.py3-none-manylinux2010_x86_64.whl (21.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.5/21.5 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (1.15.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (3.7.0)\nRequirement already satisfied: absl-py>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (0.15.0)\nRequirement already satisfied: flatbuffers>=1.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (1.12)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (1.6.3)\nCollecting tensorflow-estimator<2.9,>=2.8\n  Downloading tensorflow_estimator-2.8.0-py2.py3-none-any.whl (462 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (1.1.2)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (3.3.0)\nCollecting keras<2.9,>=2.8.0rc0\n  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n  Downloading tensorflow_io_gcs_filesystem-0.29.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (1.1.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (4.1.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (0.37.1)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (0.4.6)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (1.35.0)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (1.8.1)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (2.28.1)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (3.3.7)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (2.2.2)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (0.6.1)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (4.8)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (0.2.7)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (1.3.1)\nRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (4.13.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (2022.9.24)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (2.1.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (2.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (3.8.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow_text==2.8.2) (3.2.0)\nInstalling collected packages: tensorflow-estimator, libclang, keras, tensorflow-io-gcs-filesystem, tensorboard, tensorflow, tensorflow_text\n  Attempting uninstall: tensorflow-estimator\n    Found existing installation: tensorflow-estimator 2.6.0\n    Uninstalling tensorflow-estimator-2.6.0:\n      Successfully uninstalled tensorflow-estimator-2.6.0\n  Attempting uninstall: keras\n    Found existing installation: keras 2.6.0\n    Uninstalling keras-2.6.0:\n      Successfully uninstalled keras-2.6.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.10.1\n    Uninstalling tensorboard-2.10.1:\n      Successfully uninstalled tensorboard-2.10.1\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.6.4\n    Uninstalling tensorflow-2.6.4:\n      Successfully uninstalled tensorflow-2.6.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5, but you have tensorflow 2.8.4 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.8.4 which is incompatible.\ntensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.8.4 which is incompatible.\ntensorflow-io 0.21.0 requires tensorflow<2.7.0,>=2.6.0, but you have tensorflow 2.8.4 which is incompatible.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, but you have tensorflow-io-gcs-filesystem 0.29.0 which is incompatible.\npytorch-lightning 1.7.7 requires tensorboard>=2.9.1, but you have tensorboard 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed keras-2.8.0 libclang-15.0.6.1 tensorboard-2.8.0 tensorflow-2.8.4 tensorflow-estimator-2.8.0 tensorflow-io-gcs-filesystem-0.29.0 tensorflow_text-2.8.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"## Installing Dependencies","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow_text as text\nimport tensorflow_hub as hub\n\n%matplotlib inline\nsns.set_style('whitegrid')","metadata":{"execution":{"iopub.status.busy":"2023-01-15T06:10:03.181410Z","iopub.execute_input":"2023-01-15T06:10:03.181733Z","iopub.status.idle":"2023-01-15T06:10:07.333494Z","shell.execute_reply.started":"2023-01-15T06:10:03.181703Z","shell.execute_reply":"2023-01-15T06:10:07.332528Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Loading the model and dataset","metadata":{}},{"cell_type":"code","source":"# loading the dataset\ntrain = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv')\ntest = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-01-15T06:10:07.334735Z","iopub.execute_input":"2023-01-15T06:10:07.335622Z","iopub.status.idle":"2023-01-15T06:10:10.432985Z","shell.execute_reply.started":"2023-01-15T06:10:07.335591Z","shell.execute_reply":"2023-01-15T06:10:10.431979Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# loading the keras model\nload_model = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\nmodel = tf.keras.models.load_model('/kaggle/input/commenttoxicitymodel/comment-toxicity-bert.h5', \n                                       custom_objects={'KerasLayer':hub.KerasLayer},\n                                       options=load_model)\nprint(\"Model loaded Successfully!\")","metadata":{"execution":{"iopub.status.busy":"2023-01-15T06:10:10.435525Z","iopub.execute_input":"2023-01-15T06:10:10.435986Z","iopub.status.idle":"2023-01-15T06:10:36.631470Z","shell.execute_reply.started":"2023-01-15T06:10:10.435950Z","shell.execute_reply":"2023-01-15T06:10:36.630496Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Model loaded Successfully!\n","output_type":"stream"}]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-15T06:10:36.632964Z","iopub.execute_input":"2023-01-15T06:10:36.633331Z","iopub.status.idle":"2023-01-15T06:10:36.657762Z","shell.execute_reply.started":"2023-01-15T06:10:36.633296Z","shell.execute_reply":"2023-01-15T06:10:36.656207Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                 id                                       comment_text  toxic  \\\n0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n\n   severe_toxic  obscene  threat  insult  identity_hate  \n0             0        0       0       0              0  \n1             0        0       0       0              0  \n2             0        0       0       0              0  \n3             0        0       0       0              0  \n4             0        0       0       0              0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000997932d777bf</td>\n      <td>Explanation\\nWhy the edits made under my usern...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000103f0d9cfb60f</td>\n      <td>D'aww! He matches this background colour I'm s...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000113f07ec002fd</td>\n      <td>Hey man, I'm really not trying to edit war. It...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0001b41b1c6bb37e</td>\n      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0001d958c54c6e35</td>\n      <td>You, sir, are my hero. Any chance you remember...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-15T06:10:36.659036Z","iopub.execute_input":"2023-01-15T06:10:36.659479Z","iopub.status.idle":"2023-01-15T06:10:36.670143Z","shell.execute_reply.started":"2023-01-15T06:10:36.659444Z","shell.execute_reply":"2023-01-15T06:10:36.669077Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                 id                                       comment_text\n0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n3  00017563c3f7919a  :If you have a look back at the source, the in...\n4  00017695ad8997eb          I don't anonymously edit articles at all.","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00001cee341fdb12</td>\n      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0000247867823ef7</td>\n      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00013b17ad220c46</td>\n      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00017563c3f7919a</td>\n      <td>:If you have a look back at the source, the in...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00017695ad8997eb</td>\n      <td>I don't anonymously edit articles at all.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Clean the data \nI copied the code from my Github","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nimport os\nimport re\nimport string\nimport numpy as np\nimport pandas as pd\nfrom itertools import groupby\n\n## DATA PREPROCESSING\n\n# Removing duplicates, if words occured more than  2 times in comment.\ndef remove_duplicates(text_before):\n    my_dict = dict()\n    text_after = list()\n    for word in text_before.split():\n        if word not in my_dict.keys():\n            my_dict[word] = 1\n        else:\n            my_dict[word] = my_dict[word] + 1\n    \n    for key,value in my_dict.items():\n        if value>=2:\n            text_after.append(key)\n        else:\n            text_after.append(key)\n    return \" \".join(text_after)\n\n\ndef denoise_text(text):\n    \"\"\"Make text lowercase, remove text in square brackets, remove links,remove punctuation\n    and remove stop words containing numbers\"\"\"\n    text = text.lower()                                            # Converts the text to lowercase using regex \n    text = re.sub(r\"\\[.*?\\]\",\"\",text)                              # Replace's the text into 'nothing\" if text is present inside squre brackets.\n    text = re.sub(\"https?://\\S+|www\\.\\S+\",\"\",text)                 # Removes the links from the comments.\n    text = re.sub(\"<.*?>+\",\"\",text)                                # Remove unwanted\n    text = re.sub(\"[%s]\" % re.escape(string.punctuation),\"\",text)  # Remove punctuations\n    text = re.sub(\"\\n\",\"\",text)                                    # Remove next line symbols '\\n'\n    text = re.sub(\"\\w*\\d\\w*\",\"\",text)                              # Takes only albhabet and digits.\n    return text\n\n## STOPWORDS\nen_stop_words = stopwords.words('english')\nmore_stopwords = ['u', 'im', 'c', 'cu']\nstop_words = en_stop_words + more_stopwords\n\ndef remove_stopwords(text):\n    text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n    return text\n\n# Stemming \n'''\n    using snowballstemmer which is better than simple stemming \n    we are not using lemmaization because here we are looking for \n    a performance where time matters.  \n    In training we are using BERT to it can understand the sentiments behinf comment_text.\n'''\nstemmer = nltk.SnowballStemmer(\"english\")\n\ndef stemm_text(text):\n    text = ' '.join(stemmer.stem(word) for word in text.split(' '))\n    return text\n\n'''\n    As we are using BERT for predictive analysis, \n    no need to do stemming or lemmatization\n    \n    Bert uses BPE (Byte- Pair Encoding to shrink its vocab size), \n    so words like run and running will ultimately be decoded to run + ##ing. \n    So it's better not to convert running into run because, in some NLP problems, \n    you need that information.\n'''\n\n## Creating one parent function \n\ndef text_data_cleaning(text):\n    text = remove_duplicates(text)\n    text = denoise_text(text)\n    text = remove_stopwords(text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-01-15T06:10:36.671864Z","iopub.execute_input":"2023-01-15T06:10:36.672639Z","iopub.status.idle":"2023-01-15T06:10:37.686032Z","shell.execute_reply.started":"2023-01-15T06:10:36.672583Z","shell.execute_reply":"2023-01-15T06:10:37.685081Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"## applying cleaning on test and train\n\ntrain['comment_text_clean'] = train['comment_text'].apply(lambda text: text_data_cleaning(text))\ntest['comment_text_clean'] = test['comment_text'].apply(lambda text: text_data_cleaning(text))","metadata":{"execution":{"iopub.status.busy":"2023-01-15T06:10:37.687603Z","iopub.execute_input":"2023-01-15T06:10:37.687980Z","iopub.status.idle":"2023-01-15T06:11:34.955512Z","shell.execute_reply.started":"2023-01-15T06:10:37.687943Z","shell.execute_reply":"2023-01-15T06:11:34.954460Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"## Splitting data into train data and target data\nfrom sklearn.model_selection import train_test_split\nlist_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ntarget_data = train[list_classes]\ntrain_data = train['comment_text_clean']\n\nX_train, X_test, y_train, y_test = train_test_split(train_data, target_data, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-01-15T06:11:34.957127Z","iopub.execute_input":"2023-01-15T06:11:34.957527Z","iopub.status.idle":"2023-01-15T06:11:35.014763Z","shell.execute_reply.started":"2023-01-15T06:11:34.957488Z","shell.execute_reply":"2023-01-15T06:11:35.013764Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"## loading a model\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-01-15T06:11:35.018499Z","iopub.execute_input":"2023-01-15T06:11:35.019173Z","iopub.status.idle":"2023-01-15T06:11:35.064511Z","shell.execute_reply.started":"2023-01-15T06:11:35.019133Z","shell.execute_reply":"2023-01-15T06:11:35.063731Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Model: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n text (InputLayer)              [(None,)]            0           []                               \n                                                                                                  \n keras_layer (KerasLayer)       {'input_word_ids':   0           ['text[0][0]']                   \n                                (None, 128),                                                      \n                                 'input_type_ids':                                                \n                                (None, 128),                                                      \n                                 'input_mask': (Non                                               \n                                e, 128)}                                                          \n                                                                                                  \n keras_layer_1 (KerasLayer)     {'sequence_output':  109482241   ['keras_layer[0][0]',            \n                                 (None, 128, 768),                'keras_layer[0][1]',            \n                                 'default': (None,                'keras_layer[0][2]']            \n                                768),                                                             \n                                 'encoder_outputs':                                               \n                                 [(None, 128, 768),                                               \n                                 (None, 128, 768),                                                \n                                 (None, 128, 768),                                                \n                                 (None, 128, 768),                                                \n                                 (None, 128, 768),                                                \n                                 (None, 128, 768),                                                \n                                 (None, 128, 768),                                                \n                                 (None, 128, 768),                                                \n                                 (None, 128, 768),                                                \n                                 (None, 128, 768),                                                \n                                 (None, 128, 768),                                                \n                                 (None, 128, 768)],                                               \n                                 'pooled_output': (                                               \n                                None, 768)}                                                       \n                                                                                                  \n dropout (Dropout)              (None, 768)          0           ['keras_layer_1[0][13]']         \n                                                                                                  \n output (Dense)                 (None, 6)            4614        ['dropout[0][0]']                \n                                                                                                  \n==================================================================================================\nTotal params: 109,486,855\nTrainable params: 4,614\nNon-trainable params: 109,482,241\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{}},{"cell_type":"code","source":"## y_pred\ny_pred = model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-01-15T06:11:35.065507Z","iopub.execute_input":"2023-01-15T06:11:35.065868Z","iopub.status.idle":"2023-01-15T06:15:25.575927Z","shell.execute_reply.started":"2023-01-15T06:11:35.065822Z","shell.execute_reply":"2023-01-15T06:15:25.574874Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"y_pred[0]","metadata":{"execution":{"iopub.status.busy":"2023-01-15T06:15:25.577664Z","iopub.execute_input":"2023-01-15T06:15:25.578032Z","iopub.status.idle":"2023-01-15T06:15:25.587261Z","shell.execute_reply.started":"2023-01-15T06:15:25.577993Z","shell.execute_reply":"2023-01-15T06:15:25.586349Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"array([0.4170251 , 0.00927003, 0.13616456, 0.00258452, 0.16567819,\n       0.0514953 ], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"# Convert predictions to binary format ( 0 or 1)\ny_pred = (y_pred>0.5).astype(int)","metadata":{"execution":{"iopub.status.busy":"2023-01-15T06:15:25.588790Z","iopub.execute_input":"2023-01-15T06:15:25.589860Z","iopub.status.idle":"2023-01-15T06:15:25.594962Z","shell.execute_reply.started":"2023-01-15T06:15:25.589825Z","shell.execute_reply":"2023-01-15T06:15:25.594040Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Different Multi-label classification metrices","metadata":{}},{"cell_type":"markdown","source":"#### Accuracy Score","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: {:.2f}%\".format(accuracy * 100))","metadata":{"execution":{"iopub.status.busy":"2023-01-15T06:15:25.596330Z","iopub.execute_input":"2023-01-15T06:15:25.597194Z","iopub.status.idle":"2023-01-15T06:15:25.619336Z","shell.execute_reply.started":"2023-01-15T06:15:25.597156Z","shell.execute_reply":"2023-01-15T06:15:25.618505Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Accuracy: 90.48%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Precision and Recall Score","metadata":{}},{"cell_type":"code","source":"## Precision/Recall Score\nfrom sklearn.metrics import precision_score, recall_score\nprecision_score = precision_score(y_test, y_pred, average='micro')\nrecall_score = recall_score(y_test, y_pred, average='micro')\n\nprint(\"Precision: {:.2f}%\".format(precision_score * 100))\nprint(\"Recall: {:.2f}%\".format(recall_score * 100))","metadata":{"execution":{"iopub.status.busy":"2023-01-15T06:15:25.620718Z","iopub.execute_input":"2023-01-15T06:15:25.621074Z","iopub.status.idle":"2023-01-15T06:15:25.659842Z","shell.execute_reply.started":"2023-01-15T06:15:25.621027Z","shell.execute_reply":"2023-01-15T06:15:25.658863Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Precision: 82.32%\nRecall: 35.54%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- When a model has a high precision and low recall, it means that the model is able to identify a high proportion of true positive examples among the positive examples it predicts, but it also predicts many negative examples as positive.","metadata":{}},{"cell_type":"markdown","source":"#### Jaccard Score","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import jaccard_score\n\n# Compute the Jaccard similarity\njaccard_score = jaccard_score(y_test, y_pred, average='samples')\n\nprint(\"Jaccard similarity: {:.2f}\".format(jaccard_score))","metadata":{"execution":{"iopub.status.busy":"2023-01-15T06:24:17.605655Z","iopub.execute_input":"2023-01-15T06:24:17.606031Z","iopub.status.idle":"2023-01-15T06:24:17.631523Z","shell.execute_reply.started":"2023-01-15T06:24:17.605999Z","shell.execute_reply":"2023-01-15T06:24:17.630421Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Jaccard similarity: 0.03\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Hamming Loss","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import hamming_loss\n\nhm_loss = hamming_loss(y_test,y_pred)\n\nprint(\"Hamming Loss: {:.2f}\".format(hm_loss))","metadata":{"execution":{"iopub.status.busy":"2023-01-15T06:26:30.246608Z","iopub.execute_input":"2023-01-15T06:26:30.247018Z","iopub.status.idle":"2023-01-15T06:26:30.264019Z","shell.execute_reply.started":"2023-01-15T06:26:30.246984Z","shell.execute_reply":"2023-01-15T06:26:30.263096Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Hamming Loss: 0.03\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Classification Report","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n# Generate the classification report\nclass_report = classification_report(y_test, y_pred, zero_division=0)\n\nprint(class_report)","metadata":{"execution":{"iopub.status.busy":"2023-01-15T06:29:36.665309Z","iopub.execute_input":"2023-01-15T06:29:36.665674Z","iopub.status.idle":"2023-01-15T06:29:36.692542Z","shell.execute_reply.started":"2023-01-15T06:29:36.665643Z","shell.execute_reply":"2023-01-15T06:29:36.691416Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.84      0.39      0.53      3056\n           1       0.64      0.15      0.24       321\n           2       0.87      0.40      0.55      1715\n           3       0.00      0.00      0.00        74\n           4       0.76      0.36      0.49      1614\n           5       0.67      0.01      0.03       294\n\n   micro avg       0.82      0.36      0.50      7074\n   macro avg       0.63      0.22      0.31      7074\nweighted avg       0.80      0.36      0.49      7074\n samples avg       0.04      0.03      0.03      7074\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Please see my GitHub repository to follow end to end MLOps project on Multilabel Text Data to detect toxicity in the comment.","metadata":{}}]}